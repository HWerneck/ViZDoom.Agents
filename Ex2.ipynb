{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egG6UOvHwzJO"
      },
      "source": [
        "#1. Mount Google Drive\n",
        "This allows the notebook to import/export data from files inside Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQGAt3jHwXFq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOMETRqZw5Fd"
      },
      "source": [
        "#2. Install ViZDoom and additional libraries\n",
        "\n",
        "ViZDoom is the platform used to train the agents.\n",
        "Stable-Baselines3 is a library specific for reinforcement learning agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIkjE1g_w7FC"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!sudo apt upgrade\n",
        "\n",
        "!sudo apt install cmake libboost-all-dev libsdl2-dev libfreetype6-dev libgl1-mesa-dev libglu1-mesa-dev libpng-dev libjpeg-dev libbz2-dev libfluidsynth-dev libgme-dev libopenal-dev zlib1g-dev timidity tar nasm\n",
        "\n",
        "!pip install git+https://github.com/mwydmuch/ViZDoom\n",
        "#!pip install vizdoom\n",
        "\n",
        "!pip install stable-baselines3\n",
        "\n",
        "!sudo apt update\n",
        "!sudo apt upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Import libraries"
      ],
      "metadata": {
        "id": "D9NPwZFWMXs0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8urk-0Upz4DG"
      },
      "outputs": [],
      "source": [
        "from vizdoom import *\n",
        "from vizdoom import Button\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import math\n",
        "\n",
        "import gym\n",
        "from gym import Env\n",
        "from gym import spaces\n",
        "\n",
        "from stable_baselines3.common import vec_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
        "\n",
        "import typing as t\n",
        "import itertools\n",
        "\n",
        "from stable_baselines3 import ppo\n",
        "from stable_baselines3.common.vec_env import VecMonitor\n",
        "from stable_baselines3.common import callbacks\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common import evaluation\n",
        "from stable_baselines3.common import policies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. The ViZDoom environment\n",
        "\n",
        "### The DoomEnvironment class\n",
        "\n",
        "The DoomEnvironment class represents an interpreter of the ViZDoom game so the SB3-Agents can work with it.\n",
        "\n",
        "The class handles the step process, the rewards and the creation of the agent's observation, through the capturing and processing of game frames and any other relevant data, and structuring it into a single observation.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### The create_environment() function\n",
        "\n",
        "The create_environment() function creates an environment object, starts and returns an instance of the game.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Possible actions\n",
        "A ViZDoom agent acts on its environment through the buttons of the game. Each button represents an action. Actions may be done alone or in combination with other actions. For example, the agent is able to move around and shoot at the same time, or turn around whilst moving.\n",
        "\n",
        "For our agent, we will limit shooting as an exclusive action. It won't be allowed to shoot while moving. Also, to limit the number of combinations available for our agent to try, as they make no sense, we will not include button combinations where mutually exclusive buttons are activated at the same time (for example, moving left and right at the same time)."
      ],
      "metadata": {
        "id": "wEJ7ZGEnS1N0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsw8xWSAUtvr"
      },
      "outputs": [],
      "source": [
        "#Buttons combinations that can not be used together\n",
        "MUTUALLY_EXCLUSIVE_BUTTON_COMBINATIONS = [\n",
        "                             [Button.MOVE_FORWARD, Button.MOVE_BACKWARD],\n",
        "                             [Button.MOVE_RIGHT, Button.MOVE_LEFT],\n",
        "                             [Button.TURN_RIGHT, Button.TURN_LEFT]\n",
        "]\n",
        "\n",
        "#Buttons that can only be used alone\n",
        "EXCLUSIVE_BUTTONS = [Button.ATTACK]\n",
        "\n",
        "def get_exclusive_button_mask(actions: np.ndarray, buttons: np.array) -> np.array:\n",
        "    exclusive_action_filter = np.isin(buttons, EXCLUSIVE_BUTTONS)\n",
        "    exclusive_action_mask = (np.any(actions.astype(bool) & exclusive_action_filter, axis = -1)) & (np.sum(actions, axis = -1) > 1)\n",
        "    return exclusive_action_mask\n",
        "\n",
        "def get_mutually_exclusive_button_mask(actions: np.ndarray, buttons: np.array) -> np.array:\n",
        "    mutually_exclusive_action_filter = np.array([np.isin(buttons, invalid_combination) for invalid_combination in MUTUALLY_EXCLUSIVE_BUTTON_COMBINATIONS])\n",
        "    mutually_exclusive_action_mask = np.any(np.sum(\n",
        "                                             (actions[:, np.newaxis, :] * mutually_exclusive_action_filter.astype(int)),\n",
        "                                              axis = -1) > 1, axis = -1)\n",
        "    return mutually_exclusive_action_mask\n",
        "\n",
        "def get_available_actions(buttons: np.array) -> t.List[t.List[float]]:\n",
        "    #Create list of all possible action combinations (2^n combinations, with n being the number of available buttons)\n",
        "    action_combinations = np.array([list(seq) for seq in itertools.product([0.0, 1.0], repeat=len(buttons))])\n",
        "\n",
        "    #Create a mask that filters unathorized action combinations\n",
        "    action_filter_mask = (get_exclusive_button_mask(action_combinations, buttons)\n",
        "                         | get_mutually_exclusive_button_mask(action_combinations, buttons))\n",
        "\n",
        "    possible_actions = action_combinations[~action_filter_mask]\n",
        "    possible_actions = possible_actions[np.sum(possible_actions, axis=1) > 0]  # Prevent idling\n",
        "    return possible_actions.tolist()\n",
        "\n",
        "Frame = np.ndarray\n",
        "\n",
        "class DoomEnvironment (Env):\n",
        "    \"\"\"Wrapper environment following OpenAI's gym interface for a ViZDoom game instance.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 game: DoomGame,\n",
        "                 frame_skip: int = 4,\n",
        "                 screen_type: int = 0,\n",
        "                 limit_boxes: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.game = game\n",
        "\n",
        "        # Define some test variables\n",
        "        self.screen_type = screen_type\n",
        "        self.limit_boxes = limit_boxes\n",
        "        self.frame_skip = frame_skip\n",
        "        self.empty_frame = np.zeros((240, 320, 3), dtype=np.uint8)\n",
        "        self.player_color = [255, 255, 255]\n",
        "        self.monster_color = [255, 0, 0]\n",
        "        self.key_color = [0, 255, 0]\n",
        "        self.wall_color = [0, 0, 0]\n",
        "        self.floor_obj_color = [64, 64, 64]\n",
        "\n",
        "        self.player_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
        "        self.player_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
        "\n",
        "        self.distance_coeff = 0.2\n",
        "        ini_objects = self.game.get_state().objects\n",
        "        ini_distance = 0\n",
        "        for ini_obj in ini_objects:\n",
        "            if (ini_obj.name == 'RedSkull'):\n",
        "                ini_distance = self.calculate_object_distance(ini_obj.position_x, ini_obj.position_y)\n",
        "        self.distance = ini_distance\n",
        "        self.last_distance = ini_distance\n",
        "        self.ammo_coeff = 0.1\n",
        "        self.ammo = self.game.get_game_variable(GameVariable.AMMO2)\n",
        "        self.last_ammo = self.ammo\n",
        "        self.health_coeff = 1\n",
        "        self.health = self.game.get_game_variable(GameVariable.HEALTH)\n",
        "        self.last_health = self.health\n",
        "\n",
        "        # Determine observation space\n",
        "        self.state = self.create_observation()\n",
        "        self.observation_space = spaces.Box(low = 0, high = 255, shape = self.state.shape, dtype = np.uint8)\n",
        "\n",
        "        # Determine action space\n",
        "        self.possible_actions = get_available_actions(np.array(\n",
        "                                [Button.ATTACK, Button.MOVE_FORWARD, Button.MOVE_RIGHT, Button.MOVE_LEFT,\n",
        "                                 Button.MOVE_BACKWARD, Button.TURN_RIGHT, Button.TURN_LEFT]))\n",
        "        #self.possible_actions = np.eye(7).tolist()\n",
        "        self.action_space = spaces.Discrete(len(self.possible_actions))\n",
        "\n",
        "        return\n",
        "\n",
        "    def reset(self) -> Frame:\n",
        "        \"\"\"Resets the environment.\n",
        "        Returns:\n",
        "            The initial state of the new environment.\n",
        "        \"\"\"\n",
        "        self.game.new_episode()\n",
        "        self.state = self.create_observation()\n",
        "        return self.state\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.game.close()\n",
        "        return\n",
        "\n",
        "    def step(self, action: int) -> t.Tuple[Frame, int, bool, t.Dict]:\n",
        "        \"\"\"Apply an action to the environment.\n",
        "        Args:\n",
        "            action:\n",
        "        Returns:\n",
        "            A tuple containing:\n",
        "                - A numpy ndarray containing the current environment state.\n",
        "                - The reward obtained by applying the provided action.\n",
        "                - A boolean flag indicating whether the episode has ended.\n",
        "                - An empty info dict.\n",
        "        \"\"\"\n",
        "        reward = self.game.make_action(self.possible_actions[action], self.frame_skip)\n",
        "        additional_reward = self.calculate_additional_reward()\n",
        "        reward = reward + additional_reward\n",
        "        done = self.game.is_episode_finished()\n",
        "        self.state = self.create_observation()\n",
        "        return self.state, reward, done, {'kills':self.game.get_game_variable(GameVariable.KILLCOUNT),\n",
        "                                          'keys':self.game.get_game_variable(GameVariable.USER4),\n",
        "                                          'ammo':self.game.get_game_variable(GameVariable.AMMO2),\n",
        "                                          'health':self.game.get_game_variable(GameVariable.HEALTH),\n",
        "                                          'hit count':self.game.get_game_variable(GameVariable.HITCOUNT)}\n",
        "\n",
        "    def create_observation(self):\n",
        "        self.player_x = self.game.get_game_variable(GameVariable.POSITION_X)\n",
        "        self.player_y = self.game.get_game_variable(GameVariable.POSITION_Y)\n",
        "        screen = self.preprocess_frame()\n",
        "\n",
        "        observation = screen\n",
        "        #observation = {'screen': screen, 'data': data, 'scene_graph': scene_graph}\n",
        "        return observation\n",
        "\n",
        "    def calculate_object_distance(self, position_x, position_y):\n",
        "        distance = math.sqrt(math.pow((position_x - self.player_x), 2) + math.pow((position_y - self.player_y), 2))\n",
        "        return distance\n",
        "\n",
        "    def calculate_additional_reward(self):\n",
        "        additional_reward = 0\n",
        "        distance_reward = 0\n",
        "        ammo_reward = 0\n",
        "        health_reward = 0\n",
        "        if (self.game.get_state() is not None):\n",
        "            objects = self.game.get_state().objects\n",
        "            for obj in objects:\n",
        "                if (obj.name == 'RedSkull'):\n",
        "                    self.distance = self.calculate_object_distance(obj.position_x, obj.position_y)\n",
        "                    distance_reward += self.distance_coeff * (self.last_distance - self.distance)\n",
        "                    self.last_distance = self.distance\n",
        "\n",
        "            self.ammo = self.game.get_game_variable(GameVariable.AMMO2)\n",
        "            ammo_reward = self.ammo_coeff * (self.ammo - self.last_ammo)\n",
        "            self.last_ammo = self.ammo\n",
        "\n",
        "            self.health = self.game.get_game_variable(GameVariable.HEALTH)\n",
        "            health_reward += self.health_coeff * (self.health - self.last_health)\n",
        "            self.last_health = self.health\n",
        "\n",
        "            additional_reward = distance_reward + ammo_reward + health_reward\n",
        "        return additional_reward\n",
        "\n",
        "    def preprocess_frame(self):\n",
        "        \"\"\"\n",
        "        Preprocess frame before stacking it:\n",
        "            - Region-of-interest (ROI) selected from the original frame.\n",
        "            Frame is cut by 40 pixels up and down, and 30 pixels for left and right.\n",
        "            - Normalize images to interval [0,1]\n",
        "        \"\"\"\n",
        "        frame = self._get_screen_buffer_frame()\n",
        "        if (self.screen_type == 1):\n",
        "            frame = self._get_label_buffer_frame()\n",
        "\n",
        "        roi = frame[40:-40, :]\n",
        "        #roi = frame[40:-40, 30:-30]\n",
        "        resized_roi = cv2.resize(roi, None, fx = 0.5, fy = 0.5, interpolation = cv2.INTER_AREA)\n",
        "        #roi_normal = np.divide(roi, 255, dtype=np.float32)\n",
        "        #print(\"Preprocessed new frame is\", tf.shape(roi_normal), \".\")\n",
        "        return resized_roi\n",
        "\n",
        "    def _get_screen_buffer_frame(self):\n",
        "        \"\"\" Get the current game screen buffer or an empty screen buffer if episode is finished\"\"\"\n",
        "        if (self.game.is_episode_finished()):\n",
        "            return self.empty_frame\n",
        "        else:\n",
        "            return self.game.get_state().screen_buffer\n",
        "\n",
        "    def _get_label_buffer_frame(self):\n",
        "        \"\"\" Get the current game label screen buffer or an empty screen buffer if episode is finished\"\"\"\n",
        "        if (self.game.is_episode_finished()):\n",
        "            return self.empty_frame\n",
        "        else:\n",
        "            monster_value = 0\n",
        "            key_value = 0\n",
        "            labels_buffer = self.game.get_state().labels_buffer\n",
        "            frame = self.empty_frame\n",
        "            frame[labels_buffer == 0] = self.wall_color\n",
        "            frame[labels_buffer > 0] = self.floor_obj_color\n",
        "            frame[labels_buffer == 255] = self.player_color\n",
        "            if (self.game.get_state().labels is not None):\n",
        "                for label in self.game.get_state().labels:\n",
        "                    if (label.object_name == 'RedSkull'):\n",
        "                        key_value = label.value\n",
        "                        frame[labels_buffer == key_value] = self.key_color\n",
        "                    if (label.object_name == 'DoomImp' or label.object_name == 'Demon' or label.object_name == 'Cacodemon'):\n",
        "                        monster_value = label.value\n",
        "                        frame[labels_buffer == monster_value] = self.monster_color\n",
        "            return frame\n",
        "\n",
        "    def render(self, mode = 'human'):\n",
        "        pass\n",
        "\n",
        "def create_environment(scenario: str = 'basic', config: str = 'basic',\n",
        "                       #resolution: ScreenResolution = ScreenResolution.RES_320x240, color_scheme: int = ScreenFormat.RGB24,\n",
        "                       crosshair: bool = False, sync_mode: bool = True,\n",
        "                       game_display: bool = False, **kwargs) -> DoomEnvironment:\n",
        "    ### New game instance\n",
        "    game = DoomGame()\n",
        "\n",
        "    path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/'\n",
        "    scenario += '.wad'\n",
        "    config += '.cfg'\n",
        "\n",
        "    ### Adjust configuration and scenario paths\n",
        "    game.load_config(path+config)\n",
        "    game.set_doom_scenario_path(path+scenario)\n",
        "\n",
        "    game.set_screen_format(ScreenFormat.RGB24)\n",
        "    #game.set_screen_resolution(resolution)\n",
        "    #game.set_screen_format(color_scheme)\n",
        "\n",
        "    game.set_labels_buffer_enabled(True)\n",
        "    game.set_objects_info_enabled(True)\n",
        "\n",
        "    if (sync_mode == True):\n",
        "        game.set_mode(Mode.PLAYER)\n",
        "    else:\n",
        "        game.set_mode(Mode.ASYNC_PLAYER)\n",
        "    game.set_render_crosshair(crosshair)\n",
        "\n",
        "    ### Google Colab does not support the video output of ViZDoom. The following line is needed for the environment to be run.\n",
        "    game.set_window_visible(game_display)\n",
        "\n",
        "    #Initiate the game\n",
        "    game.init()\n",
        "\n",
        "    return DoomEnvironment(game, **kwargs)\n",
        "\n",
        "def create_vec_env(n_envs = 1, **kwargs) -> VecTransposeImage:\n",
        "    return VecTransposeImage(DummyVecEnv([lambda: create_environment(**kwargs)] * n_envs))\n",
        "\n",
        "def create_eval_vec_env(**kwargs) -> VecTransposeImage:\n",
        "    return create_vec_env(n_envs = 1, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. The play_env() function\n",
        "\n",
        "This function is responsible for running an agent with agent_args parameters in a Doom game with env_args parameters.\n",
        "\n",
        "The training data is output to a csv file containing the variables listed within the info_keywords parameter."
      ],
      "metadata": {
        "id": "5ADChLo0S9yF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZzB3nlRU1uq"
      },
      "outputs": [],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    print('Number of trainable parameters: {:,}'.format(sum(p.numel() for p in model.policy.parameters() if p.requires_grad)))\n",
        "    return\n",
        "\n",
        "def play_env(env_args, agent_args, n_envs, timesteps, callbacks, log_name, eval_freq = None, init_func = None):\n",
        "\n",
        "    csv_log_path = f'/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/Data/{log_name}'\n",
        "    csv_eval_log_path = f'/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/Data/{log_name}_eval'\n",
        "\n",
        "    #Create the environments\n",
        "    env = create_vec_env(n_envs, **env_args)\n",
        "    env = VecMonitor(env, csv_log_path, info_keywords=('kills', 'keys', 'ammo', 'health', 'hit count'))\n",
        "\n",
        "    #Build the agent\n",
        "    agent = ppo.PPO(policies.ActorCriticCnnPolicy, env, tensorboard_log = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/tensorboard', seed = 0, **agent_args)\n",
        "    count_trainable_parameters(agent)\n",
        "\n",
        "    #Optional processing on the agent\n",
        "    if init_func is not None:\n",
        "        init_func(agent)\n",
        "\n",
        "    #Optional evalutation callback\n",
        "    eval_env_args = env_args\n",
        "    #eval_env_args['frame_skip'] = 1\n",
        "    if eval_freq is not None:\n",
        "        eval_env = create_eval_vec_env(**eval_env_args)\n",
        "        eval_env = VecMonitor(eval_env, csv_eval_log_path, info_keywords=('kills', 'keys', 'ammo', 'health', 'hit count'))\n",
        "\n",
        "        callbacks.append(EvalCallback(\n",
        "                                  eval_env,\n",
        "                                  n_eval_episodes = 10,\n",
        "                                  eval_freq = eval_freq,\n",
        "                                  log_path = f'/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/evaluations/{log_name}',\n",
        "                                  best_model_save_path = f'/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/{log_name}'))\n",
        "\n",
        "    #Train the agent\n",
        "    agent.learn(total_timesteps = timesteps, tb_log_name = log_name, callback = callbacks)\n",
        "\n",
        "    #Finishing\n",
        "    env.close()\n",
        "    if eval_freq is not None:\n",
        "        eval_env.close()\n",
        "\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. GIF maker function"
      ],
      "metadata": {
        "id": "L58RVeUgTCtK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq_OYj-_U69H"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "\n",
        "def make_gif(agent, file_path, gif_env_args):\n",
        "    env = create_vec_env(scenario = gif_env_args['scenario'],\n",
        "                         config = gif_env_args['config'],\n",
        "                         #resolution = gif_env_args['resolution'],\n",
        "                         #color_scheme = gif_env_args['color_scheme'],\n",
        "                         frame_skip = 1,\n",
        "                         screen_type = gif_env_args['screen_type'],\n",
        "                         limit_boxes = gif_env_args['limit_boxes'],\n",
        "                         crosshair = gif_env_args['crosshair'],\n",
        "                         sync_mode = gif_env_args['sync_mode'],\n",
        "                         game_display = gif_env_args['game_display'])\n",
        "    env.venv.envs[0].game.set_seed(0)\n",
        "\n",
        "    images = []\n",
        "\n",
        "    for i in range(10):\n",
        "        obs = env.reset()\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, _ = agent.predict(obs)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            if gif_env_args['screen_type'] == 0:\n",
        "                images.append(env.venv.envs[0].game.get_state().screen_buffer)\n",
        "            elif gif_env_args['screen_type'] == 1:\n",
        "                label_frame = env.venv.envs[0]._get_label_buffer_frame()\n",
        "                images.append(label_frame)\n",
        "\n",
        "\n",
        "    imageio.mimsave(file_path, images, fps = 25)\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Game parameters\n",
        "\n",
        "Parameters to be used when creating the environments, the agents and the GIFs."
      ],
      "metadata": {
        "id": "Ah8hLE07TQLb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5DZROTlVWcG"
      },
      "outputs": [],
      "source": [
        "env_args = {\n",
        "    # Scenario to play\n",
        "    'scenario': 'env2',\n",
        "    # Configuration file with game rules\n",
        "    'config': 'env2_e',\n",
        "    # Screen resolution\n",
        "    #'resolution': ScreenResolution.RES_320x240,\n",
        "    # Screen coloring scheme\n",
        "    #'color_scheme': ScreenFormat.RGB24,\n",
        "    # Number of steps for which the last action is repeated\n",
        "    'frame_skip': 4,\n",
        "    # Screem type\n",
        "    #'screen_type': 0,\n",
        "    'screen_type': 1,\n",
        "    # Box representing the visual limits of each game entity\n",
        "    'limit_boxes': False,\n",
        "    # Render weapon crosshair?\n",
        "    'crosshair': False,\n",
        "    # If true, the game will be in synchronous mode, which waits for the agent to pick an action to advance the environment;\n",
        "    # if false, the environment will not wait on the agent\n",
        "    'sync_mode': True,\n",
        "    # Game window display\n",
        "    'game_display': False\n",
        "}\n",
        "\n",
        "agent_args = {\n",
        "    'n_steps': 4096,\n",
        "    'n_epochs': 3,\n",
        "    'learning_rate': 1e-4,\n",
        "    'batch_size': 32,\n",
        "    'policy_kwargs': {'features_extractor_kwargs': {'features_dim': 128}}\n",
        "}\n",
        "\n",
        "gif_env_args = {\n",
        "    #Scenario to play\n",
        "    'scenario': 'env2',\n",
        "    #Configuration file with game rules\n",
        "    'config': 'env2_e',\n",
        "    # Screen resolution\n",
        "    #'resolution': ScreenResolution.RES_640x480,\n",
        "    # Screen coloring scheme\n",
        "    #'color_scheme': ScreenFormat.RGB24,\n",
        "    # Number of steps for which the last action is repeated\n",
        "    'frame_skip': 1,\n",
        "    # Screen type\n",
        "    #'screen_type': 0,\n",
        "    'screen_type': 1,\n",
        "    # Box representing the visual limits of each game entity\n",
        "    'limit_boxes': False,\n",
        "    # Render weapon crosshair?\n",
        "    'crosshair': False,\n",
        "    # If true, the game will be in synchronous mode, which waits for the agent to pick an action to advance the environment;\n",
        "    # if false, the environment will not wait on the agent\n",
        "    'sync_mode': True,\n",
        "    # Game window display\n",
        "    'game_display': False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Running the agents\n",
        "\n",
        "Agents are trained according to the respective session parameters, evaluated at a set frequency, and then both the best and the final models are run for making each one GIF."
      ],
      "metadata": {
        "id": "lZemvwitTVO0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN-OuBiNtxTE"
      },
      "outputs": [],
      "source": [
        "#Training and playing in the environment\n",
        "agent1 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels1', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s6xs1GUVm-S"
      },
      "outputs": [],
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels1.gif'\n",
        "make_gif(agent = agent1, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels1.gif'\n",
        "agent1 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels1/best_model.zip')\n",
        "make_gif(agent = agent1, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQJCLM6guoTr"
      },
      "outputs": [],
      "source": [
        "agent2 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels2', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels2.gif'\n",
        "make_gif(agent = agent2, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels2.gif'\n",
        "agent2 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels2/best_model.zip')\n",
        "make_gif(agent = agent2, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "Qz8pXiWqZRDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EiMOGoMmRu0"
      },
      "outputs": [],
      "source": [
        "agent3 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels3', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels3.gif'\n",
        "make_gif(agent = agent3, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels3.gif'\n",
        "agent3 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels3/best_model.zip')\n",
        "make_gif(agent = agent3, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "efXlCf8RZXEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICkpDiBmmTcH"
      },
      "outputs": [],
      "source": [
        "agent4 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels4', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels4.gif'\n",
        "make_gif(agent = agent4, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels4.gif'\n",
        "agent4 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels4/best_model.zip')\n",
        "make_gif(agent = agent4, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "jB9FlsqgZfDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UjCbKQ3mdQJ"
      },
      "outputs": [],
      "source": [
        "agent5 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels5', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels5.gif'\n",
        "make_gif(agent = agent5, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels5.gif'\n",
        "agent5 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels5/best_model.zip')\n",
        "make_gif(agent = agent5, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "7trgMB-_Zysn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZMvJyF2mbU-"
      },
      "outputs": [],
      "source": [
        "agent6 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels6', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels6.gif'\n",
        "make_gif(agent = agent6, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels6.gif'\n",
        "agent6 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels6/best_model.zip')\n",
        "make_gif(agent = agent6, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "z5WCXdI2aBmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHYDV-f_maH-"
      },
      "outputs": [],
      "source": [
        "agent7 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels7', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels7.gif'\n",
        "make_gif(agent = agent7, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels7.gif'\n",
        "agent7 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels7/best_model.zip')\n",
        "make_gif(agent = agent7, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "OQaW4QCWaG4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydVWsw7SmYpa"
      },
      "outputs": [],
      "source": [
        "agent8 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels8', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels8.gif'\n",
        "make_gif(agent = agent8, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels8.gif'\n",
        "agent8 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels8/best_model.zip')\n",
        "make_gif(agent = agent8, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "enHPVse1aNcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-hS4KqGmW9K"
      },
      "outputs": [],
      "source": [
        "agent9 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels9', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels9.gif'\n",
        "make_gif(agent = agent9, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels9.gif'\n",
        "agent9 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels9/best_model.zip')\n",
        "make_gif(agent = agent9, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "8e0VFqtxaUKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jaok20F8mVge"
      },
      "outputs": [],
      "source": [
        "agent10 = play_env(env_args, agent_args, n_envs = 2, timesteps = 350000, callbacks = [], log_name = 'labels10', eval_freq = 25000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/final-labels10.gif'\n",
        "make_gif(agent = agent10, file_path = gif_file_path, gif_env_args = gif_env_args)\n",
        "gif_file_path = '/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/GIFs/best-labels10.gif'\n",
        "agent10 = ppo.PPO.load('/content/drive/My Drive/ViZDoom/Agents/Mestrado/Ex2/logs/models/labels10/best_model.zip')\n",
        "make_gif(agent = agent10, file_path = gif_file_path, gif_env_args = gif_env_args)"
      ],
      "metadata": {
        "id": "YI7wez2JaZM6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}